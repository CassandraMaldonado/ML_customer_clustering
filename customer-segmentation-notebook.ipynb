{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Segmentation Analysis\n",
    "\n",
    "This notebook implements multiple clustering techniques to segment customers based on their shopping behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Import optional libraries (install if needed)\n",
    "try:\n",
    "    import umap\n",
    "    import hdbscan\n",
    "    from kmodes.kprototypes import KPrototypes\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "except ImportError:\n",
    "    print(\"Some advanced packages are not installed. You may need to install them with pip.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Data Loading and Initial Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df_original = pd.read_csv('shopping_behavior_updated.csv')\n",
    "df = df_original.copy()  # Create a working copy of the original data\n",
    "\n",
    "# Display first few rows\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get basic information\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate descriptive statistics\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Common Feature Engineering (Base Transformations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Mapping categorical frequencies to numerical values\n",
    "frequency_mapping = {\n",
    "    \"Weekly\": 52,\n",
    "    \"Fortnightly\": 26,\n",
    "    \"Bi-Weekly\": 24,\n",
    "    \"Monthly\": 12,\n",
    "    \"Every 3 Months\": 4,\n",
    "    \"Quarterly\": 4,\n",
    "    \"Annually\": 1\n",
    "}\n",
    "\n",
    "# Create a new column with numeric frequency values\n",
    "df[\"Frequency of Purchases (Numeric)\"] = df[\"Frequency of Purchases\"].map(frequency_mapping)\n",
    "\n",
    "# 3.2 RFM base features that all models will need\n",
    "df['Recency'] = df['Previous Purchases']\n",
    "df['Frequency'] = df['Frequency of Purchases (Numeric)']\n",
    "df['Monetary'] = df['Purchase Amount (USD)']\n",
    "\n",
    "# 3.3 Engagement Score based on review rating\n",
    "if \"Review Rating\" in df.columns:\n",
    "    df[\"Engagement_Score\"] = df[\"Review Rating\"]\n",
    "else:\n",
    "    df[\"Engagement_Score\"] = 0  # Default if missing\n",
    "\n",
    "# 3.4 Binary subscription status\n",
    "df[\"Subscription Status\"] = df[\"Subscription Status\"].str.lower().str.strip()\n",
    "df[\"Subscription_Status_Binary\"] = df[\"Subscription Status\"].apply(lambda x: 1 if x == \"subscribed\" else 0)\n",
    "\n",
    "# 3.5 Loyalty Score as a combination of metrics\n",
    "df[\"Loyalty_Score\"] = df[\"Frequency\"] + df[\"Recency\"] + (df[\"Subscription_Status_Binary\"] * 10)\n",
    "\n",
    "# Store the original DataFrame with basic feature engineering for future models\n",
    "df_base = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: K-means Clustering Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Create a separate copy for K-means modeling\n",
    "df_kmeans = df_base.copy()\n",
    "\n",
    "# 4.2 Feature preparation for K-means\n",
    "# Categorical variables to lowercase for consistency\n",
    "categorical_cols = ['Gender', 'Category']\n",
    "for col in categorical_cols:\n",
    "    if col in df_kmeans.columns:\n",
    "        df_kmeans[col] = df_kmeans[col].str.lower().str.strip()\n",
    "\n",
    "# 4.3 Feature selection and standardization for K-means\n",
    "features_for_kmeans = ['Recency', 'Frequency', 'Monetary']\n",
    "scaler_kmeans = StandardScaler()\n",
    "df_kmeans_scaled = scaler_kmeans.fit_transform(df_kmeans[features_for_kmeans])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.4 Find optimal K using Elbow Method\n",
    "inertia = []\n",
    "k_range = range(1, 11)\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(df_kmeans_scaled)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "# Plot Elbow Method\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(k_range, inertia, marker=\"o\")\n",
    "plt.xlabel(\"Number of Clusters (k)\")\n",
    "plt.ylabel(\"Inertia\")\n",
    "plt.title(\"Elbow Method for Optimal K\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.5 Apply K-means with optimal k\n",
    "optimal_k = 5  # Identified from elbow plot\n",
    "kmeans_model = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "cluster_labels = kmeans_model.fit_predict(df_kmeans_scaled)\n",
    "\n",
    "# Add cluster labels to the dataframe\n",
    "df_kmeans[\"KMeans_Cluster\"] = cluster_labels\n",
    "\n",
    "# 4.6 Analysis of K-means Clusters\n",
    "\n",
    "# 4.6.1 Create a DataFrame with scaled features for analysis\n",
    "df_kmeans_analysis = pd.DataFrame(df_kmeans_scaled, columns=features_for_kmeans)\n",
    "df_kmeans_analysis[\"KMeans_Cluster\"] = cluster_labels\n",
    "\n",
    "# Calculate cluster means\n",
    "cluster_means = df_kmeans_analysis.groupby(\"KMeans_Cluster\").mean()\n",
    "print(\"Cluster means (scaled features):\")\n",
    "print(cluster_means)\n",
    "\n",
    "# 4.6.2 Evaluation metrics\n",
    "silhouette = silhouette_score(df_kmeans_scaled, cluster_labels)\n",
    "davies_bouldin = davies_bouldin_score(df_kmeans_scaled, cluster_labels)\n",
    "inertia_final = kmeans_model.inertia_\n",
    "\n",
    "print(f\"Silhouette Score: {silhouette:.4f}\")\n",
    "print(f\"Davies-Bouldin Index: {davies_bouldin:.4f}\")\n",
    "print(f\"Inertia (WCSS): {inertia_final:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.6.3 Visualize cluster characteristics\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(cluster_means, annot=True, cmap=\"coolwarm\")\n",
    "plt.title(\"K-Means Cluster Centers (RFME Features)\")\n",
    "plt.show()\n",
    "\n",
    "# 4.6.4 Distribution of spending by cluster\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x=\"KMeans_Cluster\", y=\"Monetary\", data=df_kmeans)\n",
    "plt.title(\"Spending Distribution Across Clusters\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.7 Cluster Profiling\n",
    "# Define personas based on cluster characteristics\n",
    "kmeans_personas = {\n",
    "    0: \"High-Value Loyal Customer (High monetary spend with consistent frequency)\",\n",
    "    1: \"Recent Engaged Shopper (Good engagement score with recent purchases)\",\n",
    "    2: \"Frequent Low-Spender (High frequency but low monetary value per transaction)\",\n",
    "    3: \"Infrequent Low-Spender (Low frequency and low monetary value)\",\n",
    "    4: \"Dormant High-Value Customer (High recency, meaning they haven't purchased in a while)\"\n",
    "}\n",
    "\n",
    "# Map personas to clusters\n",
    "df_kmeans[\"KMeans_Persona\"] = df_kmeans[\"KMeans_Cluster\"].map(kmeans_personas)\n",
    "\n",
    "# Summarize profiles with persona labels\n",
    "cluster_profiles_kmeans = df_kmeans.groupby(\"KMeans_Cluster\")[features_for_kmeans + [\"Loyalty_Score\", \"Engagement_Score\"]].mean().reset_index()\n",
    "cluster_profiles_kmeans[\"Persona\"] = cluster_profiles_kmeans[\"KMeans_Cluster\"].map(kmeans_personas)\n",
    "\n",
    "print(\"\\nK-Means Cluster Profiles:\")\n",
    "print(cluster_profiles_kmeans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Hierarchical Clustering Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 Create a separate copy for hierarchical clustering\n",
    "df_hierarchical = df_base.copy()\n",
    "\n",
    "# 5.2 Feature selection for hierarchical clustering \n",
    "features_hierarchical = ['Recency', 'Frequency', 'Monetary']\n",
    "\n",
    "# 5.3 Feature scaling for hierarchical clustering\n",
    "scaler_hierarchical = StandardScaler()\n",
    "df_hierarchical_scaled = scaler_hierarchical.fit_transform(df_hierarchical[features_hierarchical])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.4 Plot Dendrogram to determine the number of clusters\n",
    "plt.figure(figsize=(12, 6))\n",
    "dendrogram = sch.dendrogram(sch.linkage(df_hierarchical_scaled, method=\"ward\"))\n",
    "plt.title(\"Dendrogram for Hierarchical Clustering\")\n",
    "plt.xlabel(\"Customers\")\n",
    "plt.ylabel(\"Euclidean Distance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.5 Define number of clusters based on the dendrogram\n",
    "n_clusters_hierarchical = 4 \n",
    "\n",
    "# 5.6 Apply Agglomerative Clustering\n",
    "agg_clustering = AgglomerativeClustering(n_clusters=n_clusters_hierarchical, metric=\"euclidean\", linkage=\"ward\")\n",
    "df_hierarchical[\"Hierarchical_Cluster\"] = agg_clustering.fit_predict(df_hierarchical_scaled)\n",
    "\n",
    "# 5.7 Evaluate the clustering\n",
    "silhouette_hierarchical = silhouette_score(df_hierarchical_scaled, df_hierarchical[\"Hierarchical_Cluster\"])\n",
    "davies_bouldin_hierarchical = davies_bouldin_score(df_hierarchical_scaled, df_hierarchical[\"Hierarchical_Cluster\"])\n",
    "\n",
    "# Display clustering metrics\n",
    "print(f\"Silhouette Score (Hierarchical): {silhouette_hierarchical:.4f}\")\n",
    "print(f\"Davies-Bouldin Index (Hierarchical): {davies_bouldin_hierarchical:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.8 Analyze cluster distribution\n",
    "numeric_columns = df_hierarchical.select_dtypes(include=[np.number]).columns\n",
    "print(\"\\nHierarchical Clustering - Cluster Summary:\")\n",
    "print(df_hierarchical.groupby(\"Hierarchical_Cluster\")[numeric_columns].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.9 Visualize cluster characteristics\n",
    "cluster_means_hierarchical = df_hierarchical.groupby(\"Hierarchical_Cluster\")[features_hierarchical].mean()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(cluster_means_hierarchical, annot=True, cmap=\"coolwarm\")\n",
    "plt.title(\"Cluster Profiles (Hierarchical Clustering)\")\n",
    "plt.show()\n",
    "\n",
    "# 5.10 Distribution of spending by cluster\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x=\"Hierarchical_Cluster\", y=\"Monetary\", data=df_hierarchical)\n",
    "plt.title(\"Monetary Spend Distribution Across Clusters (Hierarchical)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.11 Define personas based on cluster behaviors\n",
    "hierarchical_personas = {\n",
    "    0: \"Premium Shopper (High spending and good frequency, indicating premium buyers.)\",\n",
    "    1: \"Occasional Buyer (They spend less per transaction and have high recency, meaning they haven't purchased recently but may return occasionally.)\",\n",
    "    2: \"At-Risk Customer (Low engagement, low monetary spend, and moderate frequency. At risk of churning soon.)\",\n",
    "    3: \"High-Frequency Budget Shopper(Very high frequency, but moderate spending. They buy often but prioritize affordability over premium products.)\"\n",
    "}\n",
    "\n",
    "# Map personas to clusters\n",
    "df_hierarchical[\"Hierarchical_Persona\"] = df_hierarchical[\"Hierarchical_Cluster\"].map(hierarchical_personas)\n",
    "\n",
    "# Summarize profiles with persona labels\n",
    "cluster_profiles_hierarchical = df_hierarchical.groupby(\"Hierarchical_Cluster\")[features_hierarchical].mean().reset_index()\n",
    "cluster_profiles_hierarchical[\"Persona\"] = cluster_profiles_hierarchical[\"Hierarchical_Cluster\"].map(hierarchical_personas)\n",
    "\n",
    "print(\"\\nHierarchical Clustering - Customer Personas:\")\n",
    "print(cluster_profiles_hierarchical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: DBSCAN (Density-Based Clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 Create a separate copy for DBSCAN clustering\n",
    "df_dbscan = df_base.copy()\n",
    "\n",
    "# 6.2 Feature selection for DBSCAN\n",
    "features_dbscan = ['Recency', 'Frequency', 'Monetary']\n",
    "\n",
    "# 6.3 Feature scaling for DBSCAN using StandardScaler\n",
    "scaler_dbscan = StandardScaler()\n",
    "df_dbscan_scaled = scaler_dbscan.fit_transform(df_dbscan[features_dbscan])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.4 K-distance graph to determine epsilon\n",
    "neighbors = NearestNeighbors(n_neighbors=2)\n",
    "neighbors_fit = neighbors.fit(df_dbscan_scaled)\n",
    "distances, indices = neighbors_fit.kneighbors(df_dbscan_scaled)\n",
    "\n",
    "# Sort and plot the distances\n",
    "distances = np.sort(distances, axis=0)\n",
    "distances = distances[:, 1]\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(distances)\n",
    "plt.title(\"K-Distance Graph\")\n",
    "plt.xlabel(\"Data Points\")\n",
    "plt.ylabel(\"Epsilon\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.5 Fine-tuning around best found parameters \n",
    "eps_values = [0.58, 0.6, 0.62]  \n",
    "min_samples_values = [5, 8, 10]  \n",
    "\n",
    "dbscan_optimized_results = {}\n",
    "\n",
    "for eps in eps_values:\n",
    "    for min_samples in min_samples_values:\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        clusters = dbscan.fit_predict(df_dbscan_scaled)\n",
    "        num_clusters = len(set(clusters)) - (1 if -1 in clusters else 0)\n",
    "        num_noise = list(clusters).count(-1)\n",
    "\n",
    "        # Compute Silhouette Score (only if more than 1 cluster exists)\n",
    "        if num_clusters > 1:\n",
    "            silhouette = silhouette_score(df_dbscan_scaled, clusters)\n",
    "        else:\n",
    "            silhouette = None\n",
    "\n",
    "        dbscan_optimized_results[(eps, min_samples)] = {\n",
    "            \"Number of Clusters\": num_clusters,\n",
    "            \"Noise Points\": num_noise,\n",
    "            \"Silhouette Score\": silhouette\n",
    "        }\n",
    "\n",
    "# Display DBSCAN parameter tuning results\n",
    "print(\"\\nDBSCAN Parameter Tuning Results:\")\n",
    "for params, results in dbscan_optimized_results.items():\n",
    "    print(f\"Parameters: {params} - Clusters: {results['Number of Clusters']}, Noise Points: {results['Noise Points']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.6 Apply DBSCAN with optimal parameters    \n",
    "optimal_eps = 0.62\n",
    "optimal_min_samples = 5\n",
    "dbscan = DBSCAN(eps=optimal_eps, min_samples=optimal_min_samples)\n",
    "\n",
    "# Apply DBSCAN and add cluster labels\n",
    "df_dbscan[\"DBSCAN_Cluster\"] = dbscan.fit_predict(df_dbscan_scaled)\n",
    "\n",
    "# 6.7 Count clusters and noise points (-1 indicates noise)\n",
    "cluster_counts = df_dbscan[\"DBSCAN_Cluster\"].value_counts()\n",
    "outliers_count = cluster_counts[-1] if -1 in cluster_counts else 0\n",
    "\n",
    "# Display results\n",
    "print(\"\\nDBSCAN Clustering Results:\")\n",
    "print(\"Cluster Distribution:\\n\", cluster_counts)\n",
    "print(\"\\nNumber of Outliers Detected:\", outliers_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.8 Evaluate DBSCAN clustering (excluding noise points)\n",
    "filtered_df_dbscan = df_dbscan[df_dbscan[\"DBSCAN_Cluster\"] != -1]\n",
    "filtered_scaled_dbscan = df_dbscan_scaled[df_dbscan[\"DBSCAN_Cluster\"] != -1]\n",
    "\n",
    "# Compute silhouette score (if there are valid clusters)\n",
    "if len(set(filtered_df_dbscan[\"DBSCAN_Cluster\"])) > 1:\n",
    "    silhouette_dbscan = silhouette_score(filtered_scaled_dbscan, filtered_df_dbscan[\"DBSCAN_Cluster\"])\n",
    "    print(f\"\\nSilhouette Score for DBSCAN: {silhouette_dbscan:.4f}\")\n",
    "else:\n",
    "    silhouette_dbscan = None\n",
    "    print(\"\\nSilhouette Score not available (DBSCAN found only one cluster).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.9 Visualize DBSCAN clustering results\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=df_dbscan[\"Recency\"], y=df_dbscan[\"Monetary\"], hue=df_dbscan[\"DBSCAN_Cluster\"], palette=\"tab10\")\n",
    "plt.title(\"DBSCAN Clustering Results (Recency vs Monetary)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.10 Define personas for DBSCAN clusters\n",
    "dbscan_personas = {\n",
    "    -1: \"Noise/Outliers (Unique customer behaviors that don't fit standard patterns)\",\n",
    "    0: \"Premium Loyalist (High monetary value, moderate frequency, and high engagement.)\",\n",
    "    1: \"High-Frequency Deal Seeker (This group buys very frequently but spends moderately, likely prioritizing discounts and promotions.)\",\n",
    "    2: \"Low-Value Irregular Shopper (They have low spending, low frequency, and are not very engaged, meaning they shop infrequently and don't interact much.)\"\n",
    "}\n",
    "\n",
    "# Map personas to clusters\n",
    "df_dbscan[\"DBSCAN_Persona\"] = df_dbscan[\"DBSCAN_Cluster\"].map(dbscan_personas)\n",
    "\n",
    "# Create a profile summary with persona labels\n",
    "dbscan_profiles = df_dbscan.groupby(\"DBSCAN_Cluster\")[features_dbscan].mean().reset_index()\n",
    "dbscan_profiles[\"Persona\"] = dbscan_profiles[\"DBSCAN_Cluster\"].map(dbscan_personas)\n",
    "\n",
    "print(\"\\nDBSCAN Customer Personas:\")\n",
    "print(dbscan_profiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: RFME Analysis (Recency, Frequency, Monetary, Engagement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.1 Create a separate copy for RFME analysis\n",
    "df_rfme = df_base.copy()\n",
    "\n",
    "# 7.2 Feature selection for RFME model\n",
    "rfme_features = [\"Recency\", \"Frequency\", \"Monetary\", \"Engagement_Score\"]\n",
    "\n",
    "# 7.3 Normalize the RFME features (min-max scaling is good for this analysis)\n",
    "scaler_rfme = MinMaxScaler()\n",
    "rfme_scaled_array = scaler_rfme.fit_transform(df_rfme[rfme_features])\n",
    "df_rfme_scaled = pd.DataFrame(rfme_scaled_array, columns=rfme_features)\n",
    "\n",
    "print(\"\\nRFME Features (Normalized):\")\n",
    "print(df_rfme_scaled.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.4 Apply K-means for RFME Clustering\n",
    "\n",
    "# Use the Elbow Method to determine optimal K\n",
    "inertia_rfme = []\n",
    "for k in range(1, 11):\n",
    "    kmeans_rfme = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans_rfme.fit(df_rfme_scaled)\n",
    "    inertia_rfme.append(kmeans_rfme.inertia_)\n",
    "\n",
    "# Plot Elbow Method\n",
    "plt.figure(figsize=(8, 5))\n",
    